{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf20b83c-d4fd-41e1-9021-e3908d1d48a9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ADD8E6; border: 1px solid gray; padding: 3px\">\n",
    "    This notebook consists of 2 agentic workflows:\n",
    "        <h3>Data Generation Workflow</h3>\n",
    "        <li><b>Data Augmentation</b>: Augments the provided image dataset.</li>\n",
    "        <h3>Validation Workflow</h3>\n",
    "        <li><b>Image Validator</b>: Identifies whether a valid driver's license exists in the given image.</li>\n",
    "        <li><b>Data Extractor</b>: Extracts relevant metadata from the image.</li>\n",
    "        <li><b>Application Validator</b>: Given the extracted metadata associated with the application, uses a set of predefined rules to validate the driver's license application.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d20a007-63ad-4192-980a-5ebb5630e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Imports\n",
    "##############################################################################\n",
    "# import pysqlite3 as sqlite3\n",
    "# import sys\n",
    "# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "import operator\n",
    "from typing import Annotated, TypedDict, List, Optional, Literal\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import io\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "import mimetypes\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "load_dotenv()\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60ff1fc-e78a-42b4-8454-5f0f9613cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# State Definitions\n",
    "##############################################################################\n",
    "\n",
    "class LicenseState(TypedDict):\n",
    "    \"\"\"Enhanced state definition\"\"\"\n",
    "    image_path: str\n",
    "    \n",
    "    image_data: Optional[bytes]\n",
    "    \n",
    "    extracted_data: dict\n",
    "    \n",
    "    validation_result: dict\n",
    "    \n",
    "    retry_count: int\n",
    "    \n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    \n",
    "    result: dict\n",
    "    \n",
    "    errors: List[str]\n",
    "    \n",
    "    warnings: List[str]\n",
    "    \n",
    "    processing_time: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6cd5a4-7e1a-4334-8369-b7e31a13f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# LLMs\n",
    "##############################################################################\n",
    "\n",
    "main_llm = ChatOpenAI(\n",
    "    \n",
    "    model=os.getenv('LLAMASCOUT4_LLM_NAME'),\n",
    "    \n",
    "    api_key=os.getenv('LLAMASCOUT4_LLM_KEY'),\n",
    "    \n",
    "    base_url=os.getenv('LLAMASCOUT4_LLM_BASE'),\n",
    "\n",
    "    max_tokens = 8192,\n",
    "\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db2216f-ba2f-4790-82ca-0e0ffc339c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data_system_prompt = \"\"\"You are an expert at extracting information from U.S. driver's licenses.\n",
    "\n",
    "Extract the following fields from the provided image. Return ONLY a valid JSON object.\n",
    "\n",
    "Required fields:\n",
    "- license_number: The driver's license number (often starts with a letter)\n",
    "- first_name: First name\n",
    "- middle_name: Middle name or initial (use null if not present)\n",
    "- last_name: Last name/surname\n",
    "- date_of_birth: Format as MM/DD/YYYY\n",
    "- issue_date: License issue date (MM/DD/YYYY)\n",
    "- expiration_date: License expiration date (MM/DD/YYYY)\n",
    "\n",
    "Address fields:\n",
    "- street_address: Street number and name\n",
    "- city: City name\n",
    "- state: Two-letter state code (e.g., CA, NY, TX)\n",
    "- zip_code: 5-digit ZIP code\n",
    "\n",
    "Physical description:\n",
    "- gender: M, F, or X\n",
    "- height: Format as feet'inches\" (e.g., 5'10\")\n",
    "- weight: Weight in pounds with \"lbs\" (e.g., \"175 lbs\")\n",
    "- eye_color: Three-letter code (BRN, BLU, GRN, etc.)\n",
    "- hair_color: Three-letter code (BRN, BLK, BLN, etc.)\n",
    "\n",
    "Other:\n",
    "- license_class: Driver's license class (C, D, M, etc.)\n",
    "- restrictions: Any restrictions (e.g., \"CORRECTIVE LENSES\")\n",
    "- endorsements: Any endorsements\n",
    "- issuing_state: Full state name\n",
    "\n",
    "Use null for any field you cannot confidently extract.\n",
    "Do not make up or guess information.\n",
    "\n",
    "Return format:\n",
    "{{\n",
    "  \"license_number\": \"value\",\n",
    "  \"first_name\": \"value\",\n",
    "  ...\n",
    "}}\"\"\"\n",
    "\n",
    "extract_data_human_prompt = \"\"\"\n",
    "\"Image:\\n\\n{image}\\n\\nExtract the information as JSON.\n",
    "\"\"\"\n",
    "\n",
    "image_validation_system_prompt = \"\"\"\n",
    "You are a data quality expert. Validate this driver's license data.\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"is_valid\": true/false,\n",
    "  \"completeness_score\": 0-100,\n",
    "  \"confidence_score\": 0-100,\n",
    "  \"critical_issues\": [\"list critical problems\"],\n",
    "  \"warnings\": [\"list minor issues\"],\n",
    "  \"missing_fields\": [\"list missing required fields\"],\n",
    "  \"recommendations\": [\"improvement suggestions\"]\n",
    "}}\"\"\"\n",
    "\n",
    "image_validation_human_prompt = \"\"\"You are a data quality expert. Validate this driver's license data.\n",
    "\n",
    "Check for:\n",
    "1. Critical fields present (license_number, name, date_of_birth)\n",
    "2. Date format correctness and logical consistency\n",
    "3. State codes are valid (2 letters)\n",
    "4. ZIP codes are 5 digits\n",
    "5. Gender codes are valid (M, F, X)\n",
    "6. Expiration date is after issue date\n",
    "7. Issue date is not in the future\n",
    "8. Person is old enough to drive (check date_of_birth)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"is_valid\": true/false,\n",
    "  \"completeness_score\": 0-100,\n",
    "  \"confidence_score\": 0-100,\n",
    "  \"critical_issues\": [\"list critical problems\"],\n",
    "  \"warnings\": [\"list minor issues\"],\n",
    "  \"missing_fields\": [\"list missing required fields\"],\n",
    "  \"recommendations\": [\"improvement suggestions\"]\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e0a61b-a335-4459-a7cd-75e195131b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Nodes\n",
    "##############################################################################\n",
    "\n",
    "def load_and_preprocess_image(state: LicenseState, encode_image_bytes=False) -> LicenseState:\n",
    "    \"\"\"Node: Load and preprocess the image\"\"\"\n",
    "    print(\"‚úì STEP 1: Image Loading & Preprocessing\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    def is_valid_http_url(input_path):\n",
    "        \"\"\"Returns whether or not the input is a valid URL.\"\"\"\n",
    "\n",
    "        parsed_url = urlparse(input_path)\n",
    "\n",
    "        is_http_url = all([parsed_url.scheme in ('http', 'https'), parsed_url.netloc])\n",
    "\n",
    "        return is_http_url\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        image_path = state[\"image_path\"]\n",
    "\n",
    "        mime_type, _ = mimetypes.guess_type(image_path)\n",
    "\n",
    "        if \"image\" in mime_type:\n",
    "\n",
    "            if encode_image_bytes:\n",
    "\n",
    "                if is_valid_http_url(image_path):\n",
    "        \n",
    "                    response = requests.get(image_path)\n",
    "\n",
    "                    response.raise_for_status()\n",
    "\n",
    "                    stream_to_read = response.content\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    with open(image_path, \"rb\") as image_file:\n",
    "                        \n",
    "                        stream_to_read = image_file.read()\n",
    "    \n",
    "                img = base64.b64encode(stream_to_read).decode(\"utf-8\")\n",
    "        \n",
    "                state[\"image_data\"] = f\"data:{mime_type};base64,{img}\"\n",
    "\n",
    "            else:\n",
    "\n",
    "                state[\"image_data\"] = image_path\n",
    "                \n",
    "\n",
    "        else:\n",
    "\n",
    "            raise Exception(f\"Mime type {mime_type} not supported\")\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"‚úÖ Image loaded: time: {processing_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        \n",
    "        error_msg = f\"Image loading error: {str(e)}\"\n",
    "        \n",
    "        state[\"errors\"].append(error_msg)\n",
    "        \n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def extract_license_data_with_llm(state: LicenseState) -> LicenseState:\n",
    "    \"\"\"Node: Extract structured data\"\"\"\n",
    "    print(\"‚úì STEP 2: AI-Powered Data Extraction\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    llm = main_llm\n",
    "    \n",
    "    extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "        \n",
    "        (\"system\", extract_data_system_prompt),\n",
    "        \n",
    "        (\"human\", extract_data_human_prompt)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        chain = extraction_prompt | llm\n",
    "        \n",
    "        response = chain.invoke({\"image\": state[\"image_data\"]})\n",
    "        \n",
    "        response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "        print(response_text)\n",
    "        \n",
    "        json_text = response_text\n",
    "        if \"```json\" in response_text:\n",
    "            \n",
    "            json_text = response_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "            \n",
    "        elif \"```\" in response_text:\n",
    "            \n",
    "            json_text = response_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        extracted_data = json.loads(json_text.strip())\n",
    "            \n",
    "        state[\"extracted_data\"] = extracted_data\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "        non_null_fields = sum(1 for v in extracted_data.values() if v is not None and v != \"\")\n",
    "        \n",
    "        total_fields = len(extracted_data)\n",
    "        \n",
    "        print(f\"‚úÖ Extraction completed\")\n",
    "        \n",
    "        print(f\"   Fields extracted: {non_null_fields}/{total_fields}\")\n",
    "        \n",
    "        print(f\"   Time: {processing_time:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n   Extracted data:\")\n",
    "        \n",
    "        print(f\"   {'-'*56}\")\n",
    "        \n",
    "        for key, value in list(extracted_data.items())[:10]:\n",
    "            \n",
    "            if value:\n",
    "                \n",
    "                display_value = str(value)[:40]\n",
    "                \n",
    "                print(f\"   {key}: {display_value}\")\n",
    "                \n",
    "        print(f\"   {'-'*56}\")\n",
    "        \n",
    "        state[\"messages\"].append(AIMessage(content=response_text))\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Extraction error: {str(e)}\"\n",
    "        \n",
    "        state[\"errors\"].append(error_msg)\n",
    "        \n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        \n",
    "        state[\"extracted_data\"] = {}\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def validate_extracted_data(state: LicenseState) -> LicenseState:\n",
    "    \"\"\"Node: Validate extracted data\"\"\"\n",
    "    print(\"‚úì STEP 3: Data Validation\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    llm = main_llm\n",
    "    \n",
    "    validation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", image_validation_system_prompt),\n",
    "        (\"human\", image_validation_human_prompt)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        chain = validation_prompt | llm\n",
    "        response = chain.invoke({\n",
    "            \"data\": json.dumps(state[\"extracted_data\"], indent=2)\n",
    "        })\n",
    "        \n",
    "        response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        json_text = response_text\n",
    "        \n",
    "        if \"```json\" in response_text:\n",
    "            \n",
    "            json_text = response_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "            \n",
    "        elif \"```\" in response_text:\n",
    "            \n",
    "            json_text = response_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        validation_result = json.loads(json_text.strip())\n",
    "        \n",
    "        state[\"validation_result\"] = validation_result\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"‚úÖ Validation completed\")\n",
    "        print(f\"   Valid: {validation_result.get('is_valid', False)}\")\n",
    "        print(f\"   Completeness: {validation_result.get('completeness_score', 0):.0f}%\")\n",
    "        print(f\"   Confidence: {validation_result.get('confidence_score', 0):.0f}%\")\n",
    "        print(f\"   Time: {processing_time:.2f}s\")\n",
    "        \n",
    "        if validation_result.get('critical_issues'):\n",
    "            \n",
    "            print(f\"\\n   ‚ö†Ô∏è  Critical Issues:\")\n",
    "            \n",
    "            for issue in validation_result['critical_issues']:\n",
    "                \n",
    "                print(f\"      ‚Ä¢ {issue}\")\n",
    "        \n",
    "        if validation_result.get('warnings'):\n",
    "            \n",
    "            print(f\"\\n   ‚ö° Warnings:\")\n",
    "            \n",
    "            for warning in validation_result['warnings']:\n",
    "                \n",
    "                print(f\"      ‚Ä¢ {warning}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Validation error: {str(e)}\"\n",
    "        \n",
    "        state[\"errors\"].append(error_msg)\n",
    "        \n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        \n",
    "        state[\"validation_result\"] = {\n",
    "            \"is_valid\": False,\n",
    "            \"completeness_score\": 0,\n",
    "            \"confidence_score\": 0,\n",
    "            \"critical_issues\": [\"Validation process failed\"],\n",
    "            \"warnings\": [],\n",
    "            \"missing_fields\": [],\n",
    "            \"recommendations\": [\"Manual review required\"]\n",
    "        }\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def compile_final_result(state: LicenseState) -> LicenseState:\n",
    "    \"\"\"Node: Compile final results\"\"\"\n",
    "    print(\"‚úì STEP 4: Compiling Final Results\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    has_errors = len(state[\"errors\"]) > 0\n",
    "    \n",
    "    is_valid = state[\"validation_result\"].get(\"is_valid\", False)\n",
    "    \n",
    "    completeness = state[\"validation_result\"].get(\"completeness_score\", 0)\n",
    "    \n",
    "    if has_errors:\n",
    "        \n",
    "        status = \"error\"\n",
    "        \n",
    "    elif not is_valid or completeness < 50:\n",
    "        \n",
    "        status = \"needs_review\"\n",
    "        \n",
    "    elif completeness < 80:\n",
    "        \n",
    "        status = \"partial\"\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        status = \"success\"\n",
    "    \n",
    "    state[\"result\"] = {\n",
    "        \n",
    "        \"status\": status,\n",
    "        \n",
    "        \"extracted_data\": state[\"extracted_data\"],\n",
    "        \n",
    "        \"validation\": state[\"validation_result\"],\n",
    "        \n",
    "        \"metadata\": {\n",
    "            \n",
    "            \"processing_timestamp\": datetime.now().isoformat(),\n",
    "            \n",
    "            \"retry_count\": state.get(\"retry_count\", 0)\n",
    "        },\n",
    "        \"errors\": state[\"errors\"],\n",
    "        \n",
    "        \"warnings\": state[\"warnings\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Status: {status.upper()}\")\n",
    "    \n",
    "    print(f\"   Errors: {len(state['errors'])}\")\n",
    "    \n",
    "    print(f\"   Warnings: {len(state['warnings'])}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af32f97-bf74-444a-b442-f6d381d1d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Graph\n",
    "##############################################################################\n",
    "def create_license_extraction_graph():\n",
    "    \"\"\"Create the LangGraph workflow\"\"\"\n",
    "    workflow = StateGraph(LicenseState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    workflow.add_node(\"load_image\", load_and_preprocess_image)\n",
    "    \n",
    "    workflow.add_node(\"extract_data\", extract_license_data_with_llm)\n",
    "    \n",
    "    # workflow.add_node(\"validate_data\", validate_extracted_data)\n",
    "    \n",
    "    workflow.add_node(\"compile_result\", compile_final_result)\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.add_edge(START, \"load_image\")\n",
    "    \n",
    "    workflow.add_edge(\"load_image\", \"extract_data\")\n",
    "    \n",
    "    # workflow.add_edge(\"extract_data\", \"validate_data\")\n",
    "    \n",
    "    # workflow.add_edge(\"validate_data\", \"compile_result\")\n",
    "\n",
    "    workflow.add_edge(\"extract_data\", \"compile_result\")\n",
    "    \n",
    "    workflow.add_edge(\"compile_result\", END)\n",
    "    \n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c0788-d85a-437b-a300-b6e3e7054eae",
   "metadata": {},
   "source": [
    "### Execute Code Translation Flow\n",
    "Execute the flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc7486ea-5f5d-453c-9c50-fb19214b2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: https://raw.githubusercontent.com/agapebondservant/dla_poc/refs/heads/main/notebooks/data2/DENVER-25CAP-00000-04SVL-ID.jpeg\n",
      "Started: 2025-11-22 07:43:35\n",
      "‚úì STEP 1: Image Loading & Preprocessing\n",
      "============================================================\n",
      "‚úÖ Image loaded: time: 0.00s\n",
      "‚úì STEP 2: AI-Powered Data Extraction\n",
      "============================================================\n",
      "After analyzing the provided image, I've extracted the following information:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"license_number\": \"A123456\",\n",
      "  \"first_name\": \"JOHN\",\n",
      "  \"middle_name\": null,\n",
      "  \"last_name\": \"SMITH\",\n",
      "  \"date_of_birth\": \"01/01/1990\",\n",
      "  \"issue_date\": \"01/01/2020\",\n",
      "  \"expiration_date\": \"01/01/2025\",\n",
      "  \"street_address\": \"123 MAIN ST\",\n",
      "  \"city\": \"DENVER\",\n",
      "  \"state\": \"CO\",\n",
      "  \"zip_code\": \"80202\",\n",
      "  \"gender\": \"M\",\n",
      "  \"height\": \"5'10\\\"\",\n",
      "  \"weight\": \"170 lbs\",\n",
      "  \"eye_color\": \"BLU\",\n",
      "  \"hair_color\": \"BRN\",\n",
      "  \"license_class\": \"C\",\n",
      "  \"restrictions\": null,\n",
      "  \"endorsements\": null,\n",
      "  \"issuing_state\": \"Colorado\"\n",
      "}\n",
      "```\n",
      "\n",
      "Please let me know if you'd like me to clarify or update any of these fields.\n",
      "‚úÖ Extraction completed\n",
      "   Fields extracted: 17/20\n",
      "   Time: 1.18s\n",
      "\n",
      "   Extracted data:\n",
      "   --------------------------------------------------------\n",
      "   license_number: A123456\n",
      "   first_name: JOHN\n",
      "   last_name: SMITH\n",
      "   date_of_birth: 01/01/1990\n",
      "   issue_date: 01/01/2020\n",
      "   expiration_date: 01/01/2025\n",
      "   street_address: 123 MAIN ST\n",
      "   city: DENVER\n",
      "   state: CO\n",
      "   --------------------------------------------------------\n",
      "‚úì STEP 4: Compiling Final Results\n",
      "============================================================\n",
      "‚úÖ Status: NEEDS_REVIEW\n",
      "   Errors: 0\n",
      "   Warnings: 0\n",
      "‚úÖ EXTRACTION COMPLETE\n",
      "   Total time: 1.19s\n",
      "============================================================\n",
      "\n",
      "üìä FINAL RESULTS\n",
      "============================================================\n",
      "{\n",
      "  \"status\": \"needs_review\",\n",
      "  \"extracted_data\": {\n",
      "    \"license_number\": \"A123456\",\n",
      "    \"first_name\": \"JOHN\",\n",
      "    \"middle_name\": null,\n",
      "    \"last_name\": \"SMITH\",\n",
      "    \"date_of_birth\": \"01/01/1990\",\n",
      "    \"issue_date\": \"01/01/2020\",\n",
      "    \"expiration_date\": \"01/01/2025\",\n",
      "    \"street_address\": \"123 MAIN ST\",\n",
      "    \"city\": \"DENVER\",\n",
      "    \"state\": \"CO\",\n",
      "    \"zip_code\": \"80202\",\n",
      "    \"gender\": \"M\",\n",
      "    \"height\": \"5'10\\\"\",\n",
      "    \"weight\": \"170 lbs\",\n",
      "    \"eye_color\": \"BLU\",\n",
      "    \"hair_color\": \"BRN\",\n",
      "    \"license_class\": \"C\",\n",
      "    \"restrictions\": null,\n",
      "    \"endorsements\": null,\n",
      "    \"issuing_state\": \"Colorado\"\n",
      "  },\n",
      "  \"validation\": {},\n",
      "  \"metadata\": {\n",
      "    \"processing_timestamp\": \"2025-11-22T07:43:36.976073\",\n",
      "    \"retry_count\": 0,\n",
      "    \"total_processing_time\": 1.186342\n",
      "  },\n",
      "  \"errors\": [],\n",
      "  \"warnings\": []\n",
      "}\n",
      "\n",
      "üíæ Results saved to: license_extraction_20251122_074336.json\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Execute the Flow\n",
    "##############################################################################\n",
    "paths = [\n",
    "    (\"https://raw.githubusercontent.com/agapebondservant/dla_poc/refs/heads/main/notebooks/data2/DENVER-25CAP-00000-04SVL-ID.jpeg\", \"data2/DENVER-25CAP-00000-04SVL-ID.json\")\n",
    "]\n",
    "\n",
    "def extract_license_info(image_path: str) -> dict:\n",
    "        \"\"\"\n",
    "        Main extraction function\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "    \n",
    "        print(f\"Image: {image_path}\")\n",
    "        print(f\"Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        app = create_license_extraction_graph()\n",
    "        \n",
    "        initial_state = {\n",
    "            \"image_path\": image_path,\n",
    "            \n",
    "            \"image_data\": None,\n",
    "            \n",
    "            \"extracted_data\": {},\n",
    "            \n",
    "            \"validation_result\": {},\n",
    "            \n",
    "            \"retry_count\": 0,\n",
    "            \n",
    "            \"messages\": [],\n",
    "            \n",
    "            \"result\": {},\n",
    "            \n",
    "            \"errors\": [],\n",
    "            \n",
    "            \"warnings\": [],\n",
    "            \n",
    "            \"processing_time\": 0.0\n",
    "        }\n",
    "        \n",
    "        # Run workflow\n",
    "        final_state = app.invoke(initial_state)\n",
    "        \n",
    "        # Calculate total time\n",
    "        total_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "        final_state[\"result\"][\"metadata\"][\"total_processing_time\"] = total_time\n",
    "        \n",
    "        print(f\"‚úÖ EXTRACTION COMPLETE\")\n",
    "    \n",
    "        print(f\"   Total time: {total_time:.2f}s\")\n",
    "    \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return final_state[\"result\"]\n",
    "\n",
    "for image_path, application in paths:\n",
    "    \n",
    "    result = extract_license_info(image_path)\n",
    "    \n",
    "    print(\"üìä FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    output_file = f\"license_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        \n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6b575b5-d975-4132-938c-c50a99f80286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 09:28:58,090 - INFO - detected formats: [<InputFormat.IMAGE: 'image'>]\n",
      "2025-11-22 09:28:58,202 - INFO - Going to convert document batch...\n",
      "2025-11-22 09:28:58,203 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-22 09:28:58,204 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-22 09:28:58,204 - INFO - Accelerator device: 'mps'\n",
      "2025-11-22 09:28:59,431 - INFO - Accelerator device: 'mps'\n",
      "2025-11-22 09:29:00,148 - INFO - Processing document DENVER-25CAP-00000-04SVL-ID.jpeg\n",
      "/opt/miniconda3/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (109734912 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "2025-11-22 09:29:57,243 - INFO - Finished converting document DENVER-25CAP-00000-04SVL-ID.jpeg in 59.77 sec.\n"
     ]
    }
   ],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# def image_to_base64(image_path, encode_image_bytes=False):\n",
    "    \n",
    "#     def is_valid_http_url(input_path):\n",
    "#         \"\"\"Returns whether or not the input is a valid URL.\"\"\"\n",
    "    \n",
    "#         parsed_url = urlparse(input_path)\n",
    "    \n",
    "#         is_http_url = all([parsed_url.scheme in ('http', 'https'), parsed_url.netloc])\n",
    "    \n",
    "#         return is_http_url\n",
    "        \n",
    "    \n",
    "#     try:\n",
    "#         start_time = datetime.now()\n",
    "    \n",
    "#         mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    \n",
    "#         if \"image\" in mime_type:\n",
    "    \n",
    "#             if encode_image_bytes:\n",
    "    \n",
    "#                 if is_valid_http_url(image_path):\n",
    "        \n",
    "#                     response = requests.get(image_path)\n",
    "    \n",
    "#                     response.raise_for_status()\n",
    "    \n",
    "#                     stream_to_read = response.content\n",
    "    \n",
    "#                 else:\n",
    "                    \n",
    "#                     with open(image_path, \"rb\") as image_file:\n",
    "                        \n",
    "#                         stream_to_read = image_file.read()\n",
    "    \n",
    "#                 img = base64.b64encode(stream_to_read).decode(\"utf-8\")\n",
    "        \n",
    "#                 return f\"data:{mime_type};base64,{img}\"\n",
    "    \n",
    "#             else:\n",
    "    \n",
    "#                 return image_path\n",
    "                \n",
    "    \n",
    "#         else:\n",
    "    \n",
    "#             raise Exception(f\"Mime type {mime_type} not supported\")\n",
    "        \n",
    "#         processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "#         print(f\"‚úÖ Image loaded: time: {processing_time:.2f}s\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "        \n",
    "#         error_msg = f\"Image loading error: {str(e)}\"\n",
    "    \n",
    "#         print(f\"‚ùå {error_msg}\")\n",
    "\n",
    "#         traceback.print_exc()\n",
    "    \n",
    "#     return None\n",
    "\n",
    "# client = OpenAI(\n",
    "        \n",
    "#     api_key=os.getenv('GRANITEDOCLING_LLM_KEY'),\n",
    "    \n",
    "#     base_url=os.getenv('GRANITEDOCLING_LLM_BASE'),\n",
    "# ) \n",
    "\n",
    "# image_path = \"https://raw.githubusercontent.com/agapebondservant/dla_poc/refs/heads/main/notebooks/data2/DENVER-25CAP-00000-04SVL-ID.jpeg\"\n",
    "\n",
    "# base64_data_url = image_to_base64(image_path, encode_image_bytes=False)\n",
    "\n",
    "# if base64_data_url:\n",
    "    \n",
    "#     response = client.chat.completions.create(\n",
    "    \n",
    "#         model=os.getenv('GRANITEDOCLING_LLM_NAME'),\n",
    "        \n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\"type\": \"text\", \"text\": \"Extract all the data you can find in this image in JSON format\"},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": base64_data_url, \"detail\": \"high\"}, # Use \"detail\": \"high\" for more detailed analysis\n",
    "#                     },\n",
    "#                 ],\n",
    "#             }\n",
    "#         ],\n",
    "#         max_tokens=8192,\n",
    "\n",
    "#         temperature=0,\n",
    "#     )\n",
    "#     print(response.choices[0].message.content)\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "import pprint\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "image_source = \"https://raw.githubusercontent.com/agapebondservant/dla_poc/refs/heads/main/notebooks/data2/DENVER-25CAP-00000-04SVL-ID.jpeg\" # or a URL\n",
    "\n",
    "conversion_result = converter.convert(source=image_source)\n",
    "\n",
    "doctags_output = conversion_result.document.export_to_doctags()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4722d14f-9243-4ba9-b09a-d46ea5dfb313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doctag>\n",
      " <text>\n",
      "  <loc_134>\n",
      "   <loc_199>\n",
      "    <loc_146>\n",
      "     <loc_263>\n",
      "      a ISS: 12/10/2024\n",
      "     </loc_263>\n",
      "    </loc_146>\n",
      "   </loc_199>\n",
      "  </loc_134>\n",
      " </text>\n",
      " <text>\n",
      "  <loc_101>\n",
      "   <loc_199>\n",
      "    <loc_132>\n",
      "     <loc_282>\n",
      "      16 Sex:V is Hgt6'-01* ‚Ä¢ Class:R &amp; NONE\n",
      "     </loc_282>\n",
      "    </loc_132>\n",
      "   </loc_199>\n",
      "  </loc_101>\n",
      " </text>\n",
      " <text>\n",
      "  <loc_118>\n",
      "   <loc_287>\n",
      "    <loc_130>\n",
      "     <loc_328>\n",
      "      48 Eye: GRN\n",
      "     </loc_328>\n",
      "    </loc_130>\n",
      "   </loc_287>\n",
      "  </loc_118>\n",
      " </text>\n",
      " <picture>\n",
      "  <loc_78>\n",
      "   <loc_85>\n",
      "    <loc_400>\n",
      "     <loc_466>\n",
      "     </loc_466>\n",
      "    </loc_400>\n",
      "   </loc_85>\n",
      "  </loc_78>\n",
      " </picture>\n",
      "</doctag>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = doctags_output\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "pretty_html = soup.prettify()\n",
    "print(pretty_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354dbcb-25c4-466f-8c5d-3ba4ab700b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from docling_core.types.doc import DoclingDocument\n",
    "from docling_core.types.doc.document import DocTagsDocument\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load images\n",
    "image = load_image(\"https://raw.githubusercontent.com/agapebondservant/dla_poc/refs/heads/main/notebooks/data2/DENVER-25CAP-00000-04SVL-ID.jpeg\")\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"ibm-granite/granite-docling-258M\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"ibm-granite/granite-docling-258M\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"sdpa\",\n",
    ").to(DEVICE)\n",
    "\n",
    "# Create input messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Convert this image to docling.\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=8192)\n",
    "prompt_length = inputs.input_ids.shape[1]\n",
    "trimmed_generated_ids = generated_ids[:, prompt_length:]\n",
    "doctags = processor.batch_decode(\n",
    "    trimmed_generated_ids,\n",
    "    skip_special_tokens=False,\n",
    ")[0].lstrip()\n",
    "\n",
    "print(f\"DocTags: \\n{doctags}\\n\")\n",
    "\n",
    "\n",
    "# Populate document\n",
    "doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n",
    "# create a docling document\n",
    "doc = DoclingDocument.load_from_doctags(doctags_doc, document_name=\"Document\")\n",
    "print(f\"Markdown:\\n{doc.export_to_markdown()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bac85d95-61cd-4a30-9a81-02720b224d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 16:43:08,913 - INFO - detected formats: [<InputFormat.IMAGE: 'image'>]\n",
      "2025-11-22 16:43:08,986 - INFO - Going to convert document batch...\n",
      "2025-11-22 16:43:08,988 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-22 16:43:08,992 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-22 16:43:08,993 - INFO - Accelerator device: 'mps'\n",
      "2025-11-22 16:43:13,303 - INFO - Accelerator device: 'mps'\n",
      "2025-11-22 16:43:14,051 - INFO - Processing document DENVER-25CAP-00000-04SVL-ID.jpeg\n",
      "/opt/miniconda3/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (109734912 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "2025-11-22 16:44:14,952 - INFO - Finished converting document DENVER-25CAP-00000-04SVL-ID.jpeg in 66.57 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ISS: 12/10/2024\n",
      "\n",
      "16 Sex:V is Hgt6'-01* ‚Ä¢ Class:R &amp; NONE\n",
      "\n",
      "48 Eye: GRN\n",
      "\n",
      "<!-- image -->\n"
     ]
    }
   ],
   "source": [
    "from docling.datamodel import vlm_model_specs\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    VlmPipelineOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "source = \"https://raw.githubusercontent.com/agapebondservant/dla_poc/refs/heads/main/notebooks/data2/DENVER-25CAP-00000-04SVL-ID.jpeg\"\n",
    "\n",
    "# # 1. Classic implementation\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls=VlmPipeline,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "doc = converter.convert(source=source).document\n",
    "print(doc.export_to_markdown())\n",
    "\n",
    "# 2. VLM (on Mac via MLX)\n",
    "\n",
    "# vlm_options=InlineVlmOptions(\n",
    "#     repo_id=\"ibm-granite/granite-vision-3.2-2b\",\n",
    "#     prompt=\"Extract all the data you can from this image.\",\n",
    "#     response_format=ResponseFormat.MARKDOWN,\n",
    "#     inference_framework=InferenceFramework.TRANSFORMERS,\n",
    "#     transformers_model_type=TransformersModelType.AUTOMODEL_VISION2SEQ,\n",
    "#     supported_devices=[\n",
    "#         AcceleratorDevice.CPU,\n",
    "#         AcceleratorDevice.CUDA,\n",
    "#         AcceleratorDevice.MPS,\n",
    "#     ],\n",
    "#     scale=2.0,\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "# pipeline_options = VlmPipelineOptions(\n",
    "#     vlm_options=vlm_options #vlm_model_specs.GRANITEDOCLING_MLX,\n",
    "# )\n",
    "# converter = DocumentConverter(\n",
    "#     format_options={\n",
    "#         InputFormat.PDF: PdfFormatOption(\n",
    "#             pipeline_cls=VlmPipeline,\n",
    "#             pipeline_options=pipeline_options,\n",
    "#         ),\n",
    "#     }\n",
    "# )\n",
    "# doc = converter.convert(source=source).document\n",
    "# print(doc.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e031f43-7596-418f-af00-4a35e7daef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/tolaawofolu/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "#YOLOv10-X\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd397d2-1c1b-4526-8e44-9239cef159a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov10x.pt to 'yolov10x.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 61.4MB 19.2MB/s 3.2s3.1s<0.0s2s\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov10x.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d815115-e699-41f4-89f2-30ba25038789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
